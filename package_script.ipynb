{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "try:\n",
    "    matplotlib.use(\"Agg\")\n",
    "except Exception as e:\n",
    "    print(\"Error: could not use Agg as backend\")\n",
    "    pass\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import struct\n",
    "from bitstring import BitArray\n",
    "from scipy import signal\n",
    "from shutil import copy, rmtree\n",
    "\n",
    "sns.set_theme() # ajusta automaticamente o estilo dos gráficos\n",
    "pd.options.mode.chained_assignment = None # desabilita os avisos sobre atribuições encadeadas no Pandas\n",
    "__all__ = [\"SleepPy\", \"ColeKripke\", \"band_pass_filter\", \"activity_index\", \"bin2df\"] # especifica os nomes dos objetos que devem ser importados quando o módulo é importado usando from module import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código abaixo foi retirado do pacote SleepPy (https://github.com/elyiorgos/sleeppy/blob/master/sleeppy/sleep.py). Serão utilizadas as funções para processamento dos dados armazenados no .csv extraído do arquivo .bin gerado pelo actígrafo. A extração foi feita no aplicativo do GENEActiv.\n",
    "\n",
    "<font color=red>O código abaixo pode ter sido modificado para atender as necessidades do projeto.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ATENÇÃO:** todas as células devem ser executadas antes da instância do objeto da classe SleepPy, uma vez que a classe chama funções que são definidas posteriormente no código </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cópia completa da classe\n",
    "\n",
    "Alterações feitas:\n",
    "\n",
    "***.ix*** foi removido nas versões mais recentes do Pandas (a partir da versão 0.20.0). Para corrigir o código, .ix foi substituído por um método de indexação alternativo:\n",
    "\n",
    "- .loc (para rótulos)\n",
    "- .iloc (para índices numéricos)\n",
    "\n",
    "**ticks.label** foi substituído por **plt.xticks** para evitar o erro: *AttributeError: 'XTick' object has no attribute 'label'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepPy:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_file,\n",
    "        results_directory,\n",
    "        sampling_frequency,\n",
    "        start_buffer=\"0s\",\n",
    "        stop_buffer=\"0s\",\n",
    "        start_time=\"\",\n",
    "        stop_time=\"\",\n",
    "        run_config=0,\n",
    "        temperature_threshold=25.0,\n",
    "        minimum_rest_block=30,\n",
    "        allowed_rest_break=60,\n",
    "        minimum_rest_threshold=0.0,\n",
    "        maximum_rest_threshold=1000.0,\n",
    "        minimum_hours=6,\n",
    "        clear_intermediate_data=False,\n",
    "        aws_object=None,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \n",
    "        if aws_object is not None:\n",
    "            self.src = aws_object\n",
    "        else:\n",
    "            self.src = input_file  # save input location\n",
    "        self.extension = input_file.split(\".\")[-1]\n",
    "        self.dst = results_directory  # save output location\n",
    "        self.src_name = input_file.split(\"/\")[-1][0:-4]  # save naming convention\n",
    "        self.sub_dst = (\n",
    "            results_directory + \"/\" + self.src_name\n",
    "        )  # create output directory\n",
    "        self.fs = sampling_frequency  # save sampling frequency\n",
    "        self.window_size = 60  # define window size in seconds\n",
    "        self.band_pass_cutoff = (\n",
    "            0.25,\n",
    "            12.0,\n",
    "        )  # define the cutoffs for the band pass filter\n",
    "        self.major_rest_periods = []  # initialize a list to save the major rest periods\n",
    "        self.start_buffer = start_buffer\n",
    "        self.stop_buffer = stop_buffer\n",
    "        self.start_time = start_time\n",
    "        self.stop_time = stop_time\n",
    "        self.run_config = run_config\n",
    "        self.min_t = temperature_threshold\n",
    "        self.minimum_rest_block = minimum_rest_block\n",
    "        self.allowed_rest_break = allowed_rest_break\n",
    "        self.minimum_rest_threshold = minimum_rest_threshold\n",
    "        self.maximum_rest_threshold = maximum_rest_threshold\n",
    "        self.minimum_hours = minimum_hours\n",
    "        self.clear = clear_intermediate_data\n",
    "        self.verbose = verbose\n",
    "        self.run()  # run the package\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the full package on the provided file.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.mkdir(self.sub_dst)  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        if self.run_config <= 0:\n",
    "            # split the data into 24 hour periods\n",
    "            if self.verbose:\n",
    "                print(\"Loading data...\")\n",
    "            if \".bin\" in self.src:\n",
    "                self.split_days_geneactiv_bin()\n",
    "            elif \".csv\" in self.src:\n",
    "                self.split_days_geneactiv_csv()\n",
    "        \n",
    "        if self.run_config <= 1:\n",
    "            # extract the activity index feature\n",
    "            if self.verbose:\n",
    "                print(\"Extracting activity index...\")\n",
    "            self.extract_activity_index()\n",
    "        if self.run_config <= 2:\n",
    "            # run wear/on-body detection\n",
    "            if self.verbose:\n",
    "                print(\"Running off-body detection...\")\n",
    "            self.wear_detection()\n",
    "        if self.run_config <= 3:\n",
    "            # run major rest period detection\n",
    "            if self.verbose:\n",
    "                print(\"Detecting major rest period...\")\n",
    "            self.major_rest_period()\n",
    "        if self.run_config <= 4:\n",
    "            # run sleep wake predictions on the major rest period\n",
    "            if self.verbose:\n",
    "                print(\"Running sleep/wake predictions...\")\n",
    "            self.sleep_wake_predict()\n",
    "        if self.run_config <= 5:\n",
    "            # calculate endpoints based on the above predictions\n",
    "            if self.verbose:\n",
    "                print(\"Calculating endpoints...\")\n",
    "            self.calculate_endpoints()\n",
    "        \n",
    "        \"\"\" \n",
    "            if self.run_config == 6:\n",
    "                # generates visual reports\n",
    "                if self.verbose:\n",
    "                    print(\"Generating visual reports...\")\n",
    "                self.visualize_results()\n",
    "           \n",
    "            # aggregate results\n",
    "            if self.verbose:\n",
    "                print(\"Aggregating results...\")\n",
    "            self.aggregate_results()\n",
    "\n",
    "            # clear data\n",
    "            if self.clear:\n",
    "                if self.verbose:\n",
    "                    print(\"Clearing intermediate data...\")\n",
    "                self.clear_data()\n",
    "        \"\"\"\n",
    "        \n",
    "    def split_days_geneactiv_csv(self):\n",
    "\n",
    "        try:\n",
    "            os.mkdir(self.sub_dst + \"/raw_days\")  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        # load data and fix time_stamps\n",
    "        data = pd.read_csv(\n",
    "            self.src, # arquivo csv\n",
    "            index_col=0,\n",
    "            skiprows=100,\n",
    "            header=None,\n",
    "            names=[\"Time\", \"X\", \"Y\", \"Z\", \"LUX\", \"Button\", \"T\"],\n",
    "            usecols=[\"Time\", \"X\", \"Y\", \"Z\", \"LUX\", \"T\"],\n",
    "            dtype={\n",
    "                \"Time\": object,\n",
    "                \"X\": np.float64,\n",
    "                \"Y\": np.float64,\n",
    "                \"Z\": np.float64,\n",
    "                \"LUX\": np.int64,\n",
    "                \"Button\": bool,\n",
    "                \"T\": np.float64,\n",
    "            },\n",
    "            low_memory=False,\n",
    "        )\n",
    "        data.index = pd.to_datetime(data.index, format=\"%Y-%m-%d %H:%M:%S:%f\").values\n",
    "\n",
    "        # remove any specified time periods from the beginning and end of the file\n",
    "        data = data.loc[\n",
    "            data.index[0]\n",
    "            + pd.Timedelta(self.start_buffer) : data.index[-1]\n",
    "            - pd.Timedelta(self.stop_buffer)\n",
    "        ]\n",
    "\n",
    "        # cut to defined start and end times if specified\n",
    "        if self.start_time and self.stop_time:\n",
    "            self.start_time = pd.to_datetime(\n",
    "                self.start_time, format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "            )\n",
    "            self.stop_time = pd.to_datetime(\n",
    "                self.stop_time, format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "            )\n",
    "            data = data.loc[self.start_time : self.stop_time]\n",
    "        elif self.start_time:\n",
    "            self.start_time = pd.to_datetime(\n",
    "                self.start_time, format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "            )\n",
    "            data = data.loc[self.start_time :]\n",
    "        elif self.stop_time:\n",
    "            self.stop_time = pd.to_datetime(\n",
    "                self.stop_time, format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "            )\n",
    "            data = data.loc[: self.stop_time]\n",
    "\n",
    "\n",
    "        # Salvando dados em um csv unico contendo todos os dias\n",
    "\n",
    "        raw_data = data[['X', 'Y', 'Z', 'T']]\n",
    "\n",
    "        raw_data['Sleep'] = 0\n",
    "\n",
    "        # transformar datetime pra hora:min\n",
    "        #raw_data.index = pd.to_datetime(data.index, format=\"%Y-%m-%d %H:%M%f\").values # convertendo datetime H:M\n",
    "\n",
    "        # ler arquivo sleep_sum_total\n",
    "        # converter sleeponset_ts e wakeup_ts para datetime hora:min\n",
    "        # bater id da paciente com o id da tabela (transformar patient_id_csv para int se necessario)\n",
    "            # se o id for igual, pega o valor da coluna sleeponset_ts e wakeup_ts\n",
    "        # percorrer o df ate que o id do raw_data seja igual a sleeponset_ts \n",
    "            # condicao de parada: id = wakeup_ts\n",
    "\n",
    "        a = os.path.splitext(os.path.basename(input_file_path))\n",
    "        patient_id_csv = a[0].split('_')\n",
    "        patient_id_csv[0] # utilizar a primeira posicao\n",
    "\n",
    "        raw_data.to_csv(self.sub_dst + f\"/raw_days/{patient_id_csv[0]}_raw_data.csv\") \n",
    "\n",
    "        # split data into days from noon to noon\n",
    "        days = data.groupby(pd.Grouper(level=0, freq=\"24h\", offset=\"12h\"))\n",
    "\n",
    "        # iterate through days keeping track of the day\n",
    "        count = 0\n",
    "        for day in days:\n",
    "            # save each 24 hour day separately if there's enough data to analyze\n",
    "            df = day[1].copy()\n",
    "            available_hours = (len(df) / float(self.fs)) / 3600.0\n",
    "            if available_hours >= self.minimum_hours:\n",
    "                count += 1\n",
    "                dst = \"/raw_days/{}_day_{}.h5\".format(\n",
    "                    self.src_name, str(count).zfill(2)\n",
    "                )\n",
    "                df.to_hdf(self.sub_dst + dst, key=\"raw_geneactiv_data_24hr\", mode=\"w\")\n",
    "        return\n",
    "\n",
    "    def split_days_geneactiv_bin(self):\n",
    "\n",
    "        try:\n",
    "            os.mkdir(self.sub_dst + \"/raw_days\")  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        # load data and fix time_stamps\n",
    "        data = bin2df(self.src)\n",
    "\n",
    "        # remove any specified time periods from the beginning and end of the file\n",
    "        data = data.loc[\n",
    "            data.index[0]\n",
    "            + pd.Timedelta(self.start_buffer) : data.index[-1]\n",
    "            - pd.Timedelta(self.stop_buffer)\n",
    "        ]\n",
    "\n",
    "        # cut to defined start and end times if specified\n",
    "        if self.start_time and self.stop_time:\n",
    "            self.start_time = pd.to_datetime(\n",
    "                self.start_time, format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "            )\n",
    "            self.stop_time = pd.to_datetime(\n",
    "                self.stop_time, format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "            )\n",
    "            data = data.loc[self.start_time : self.stop_time]\n",
    "        elif self.start_time:\n",
    "            self.start_time = pd.to_datetime(\n",
    "                self.start_time, format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "            )\n",
    "            data = data.loc[self.start_time :]\n",
    "        elif self.stop_time:\n",
    "            self.stop_time = pd.to_datetime(\n",
    "                self.stop_time, format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "            )\n",
    "            data = data.loc[: self.stop_time]\n",
    "\n",
    "        # split data into days from noon to noon\n",
    "        days = data.groupby(pd.Grouper(level=0, freq=\"24h\", base=12))\n",
    "\n",
    "        # iterate through days keeping track of the day\n",
    "        count = 0\n",
    "        for day in days:\n",
    "            # save each 24 hour day separately if there's enough data to analyze\n",
    "            df = day[1].copy()\n",
    "            available_hours = (len(df) / float(self.fs)) / 3600.0\n",
    "            if available_hours >= self.minimum_hours:\n",
    "                count += 1\n",
    "                dst = \"/raw_days/{}_day_{}.h5\".format(\n",
    "                    self.src_name, str(count).zfill(2)\n",
    "                )\n",
    "                df.to_hdf(self.sub_dst + dst, key=\"raw_geneactiv_data_24hr\", mode=\"w\")\n",
    "        return\n",
    "\n",
    "    def extract_activity_index(self):\n",
    "\n",
    "        try:\n",
    "            os.mkdir(self.sub_dst + \"/activity_index_days\")  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        count = 0\n",
    "\n",
    "        # get days\n",
    "\n",
    "        \"\"\"\n",
    "            os.listdir(self.sub_dst + \"/raw_days/\"): Obtém uma lista de todos os arquivos e pastas no diretório raw_days, \n",
    "            que está localizado dentro do diretório self.sub_dst.\n",
    "\n",
    "            Para cada item (arquivo ou pasta) da lista obtida acima, o código verifica se o nome do item não contém \".DS_Store\" \n",
    "            (um arquivo oculto comum em sistemas macOS que não é relevante para processamento).\n",
    "            Se o arquivo/pasta não contiver \".DS_Store\", ele é adicionado à lista com o caminho completo self.sub_dst + \"/raw_days/\" + i.\n",
    "\n",
    "            sorted(): Após gerar a lista dos caminhos completos dos arquivos (exceto aqueles que contêm \".DS_Store\"), \n",
    "            a lista é ordenada em ordem alfabética.\n",
    "        \"\"\"\n",
    "        days = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/raw_days/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/raw_days/\")\n",
    "                #if \".DS_Store\" not in i\n",
    "                if i.endswith(\".h5\") and \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        all_activity_dataframes = []\n",
    "\n",
    "        for day in days:\n",
    "            count += 1\n",
    "\n",
    "            # load data\n",
    "            df = pd.read_hdf(day)\n",
    "            activity = [] # usada para armazenar os índices de atividade para cada janela de tempo\n",
    "            header = [\"Time\", \"activity_index\"]\n",
    "            idx = 0\n",
    "            window = int(self.window_size * self.fs) # tamanho da janela de processamento\n",
    "            incrementer = int(self.window_size * self.fs) # passo de avanço para o processamento da próxima janela\n",
    "\n",
    "            # iterate through windows\n",
    "            \"\"\"\n",
    "                1. Uma janela de dados temp é extraída das colunas \"X\", \"Y\" e \"Z\" para os sensores em um intervalo de idx até idx + window.\n",
    "                2. O tempo inicial da janela é armazenado em start_time.\n",
    "                3. O índice da janela é redefinido (temp.index = range(...)).\n",
    "            \"\"\"\n",
    "            while idx < len(df) - incrementer:\n",
    "                # preprocessing: BP Filter\n",
    "                temp = df[[\"X\", \"Y\", \"Z\"]].iloc[idx : idx + window]\n",
    "                start_time = temp.index[0]\n",
    "                temp.index = range(len(temp.index))  # reset index\n",
    "                \n",
    "                temp = band_pass_filter(\n",
    "                    temp, self.fs, bp_cutoff=self.band_pass_cutoff, order=3\n",
    "                    # filtro remove ruídos fora de uma faixa de frequência de interesse, definida por self.band_pass_cutoff\n",
    "                )\n",
    "\n",
    "                # activity index extraction\n",
    "                bp_channels = [\n",
    "                    i for i in temp.columns.values[1:] if \"bp\" in i\n",
    "                ]  # band pass filtered channels\n",
    "                activity.append(\n",
    "                    [\n",
    "                        start_time,\n",
    "                        activity_index(temp, channels=bp_channels).values[0][0],\n",
    "                        # calcula um índice de atividade para os canais filtrados (com bp no nome)\n",
    "                    ]\n",
    "                )\n",
    "                idx += incrementer # avança a janela\n",
    "\n",
    "            # save data\n",
    "            activity = pd.DataFrame(activity)\n",
    "            activity.columns = header\n",
    "            activity.set_index(\"Time\", inplace=True)\n",
    "            activity_df_csv = activity\n",
    "            # O caminho de destino é construído com base no nome da origem (self.src_name) e no número \n",
    "            # do dia, que é formatado com dois dígitos (str(count).zfill(2)).\n",
    "            dst = \"/activity_index_days/{}_activity_index_day_{}.h5\".format(\n",
    "                self.src_name, str(count).zfill(2)\n",
    "            )\n",
    "            activity.to_hdf(\n",
    "                self.sub_dst + dst, key=\"activity_index_data_24hr\", mode=\"w\"\n",
    "            )\n",
    "\n",
    "\n",
    "            ### CRIANDO ARQUIVO CSV ACTIVITY INDEX\n",
    "            # aggregate and save the major rest period for each day\n",
    "            #dst_activity = \"{}_activity_index.csv\".format(self.src_name)\n",
    "            #activity_df_csv.to_csv(\"activity_index.csv\")\n",
    "            #activity_df_csv.to_csv(self.sub_dst + \"/activity_index_days/{}_activity_index_day_{}.csv\".format(\n",
    "            #                        self.src_name, str(count).zfill(2))\n",
    "            #)\n",
    "            all_activity_dataframes.append(activity)\n",
    "\n",
    "        a = os.path.splitext(os.path.basename(input_file_path))\n",
    "        patient_id_csv = a[0].split('_')\n",
    "        patient_id_csv[0] \n",
    "\n",
    "        all_activity_data = pd.concat(all_activity_dataframes)\n",
    "        all_activity_data.to_csv(self.sub_dst + f\"/activity_index_days/{patient_id_csv[0]}_activity_index.csv\")\n",
    "\n",
    "    def wear_detection(self):\n",
    "     \n",
    "        try:\n",
    "            os.mkdir(self.sub_dst + \"/wear_detection\")  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        count = 0 # identifica o dia atual\n",
    "\n",
    "        # get days\n",
    "        days = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/raw_days/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/raw_days/\")\n",
    "                #if \".DS_Store\" not in i\n",
    "                if i.endswith(\".h5\") and \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        for day in days:\n",
    "            df = pd.read_hdf(day)[[\"X\", \"Y\", \"Z\"]]\n",
    "            count += 1\n",
    "\n",
    "            # get std based classification criteria\n",
    "            df_std = self.roll_std_60_minute(df) # calcula o desvio padrão dos sinais em janelas de 60 minutos\n",
    "\n",
    "            \"\"\"\n",
    "                Se o desvio padrão for maior ou igual a 0.013, a janela é marcada como \"usada\" (1).\n",
    "                Se for menor que 0.013, é marcada como \"não usada\" (0).\n",
    "                df_std: soma dos valores ao longo dos eixos do sensor para cada linha (ponto no tempo).\n",
    "            \"\"\"\n",
    "            df_std[df_std >= 0.013] = 1\n",
    "            df_std[df_std < 0.013] = 0\n",
    "            df_std = df_std.sum(axis=1)\n",
    "\n",
    "            # get range based classification criteria\n",
    "            df_range = self.roll_max_range_60_minute(df) # calcula o intervalo máximo (diferença entre máximos e mínimos)\n",
    "            df_range[df_range >= 0.15] = 1\n",
    "            df_range[df_range < 0.15] = 0\n",
    "            df_range = df_range.sum(axis=1)\n",
    "\n",
    "            # classify\n",
    "            \"\"\"\n",
    "                df_wear: armazena a classificação do uso do sensor. Inicialmente, todas as entradas são definidas como \"usado\" (1).\n",
    "                Para cada linha (ponto no tempo), se ambos os critérios (desvio padrão e intervalo) forem baixos (<= 1), o valor é marcado como \"não usado\" (0).\n",
    "            \"\"\"\n",
    "            df_wear = pd.DataFrame(df_std.copy()) * 0 + 1\n",
    "            df_wear.columns = [\"wear\"]\n",
    "\n",
    "            all_wear_dataframes = []\n",
    "            \n",
    "            for i in range(len(df_wear)):\n",
    "                if df_range.iloc[i] <= 1 or df_std.iloc[i] <= 1:  # Mudança de ix para iloc\n",
    "                    df_wear.iloc[i] = 0  # Mudança de ix para iloc\n",
    "\n",
    "            # save before rescoring\n",
    "            df_wear_csv = df_wear\n",
    "\n",
    "            df_wear.to_hdf(\n",
    "                self.sub_dst\n",
    "                + \"/wear_detection/wear_detection_day_{}.h5\".format(\n",
    "                    str(count).zfill(2)\n",
    "                ),\n",
    "                key=\"wear_detection_24hr\",\n",
    "                mode=\"w\",\n",
    "            ) # resultados iniciais são salvos antes da reclassificacao\n",
    "\n",
    "            # apply rescoring\n",
    "            df_wear = self.rescore(df_wear)\n",
    "            df_wear = self.rescore(df_wear)\n",
    "            df_wear = self.rescore(df_wear)\n",
    "            if count == len(days):\n",
    "                df_wear = self.rescore_last_day(df_wear) # reclassificacao especial para o ultimo dia\n",
    "\n",
    "            \n",
    "            #df_wear_resc_csv = df_wear\n",
    "            # save post rescoring\n",
    "            df_wear.to_hdf(\n",
    "                self.sub_dst\n",
    "                + \"/wear_detection/wear_detection_rescored_day_{}.h5\".format(\n",
    "                    str(count).zfill(2)\n",
    "                ),\n",
    "                key=\"wear_detection_rescored_24hr\",\n",
    "                mode=\"w\",\n",
    "            )\n",
    "\n",
    "\n",
    "            #df_wear_csv.to_csv(self.sub_dst + \"/wear_detection/wear_detection_day_{}.csv\".format(\n",
    "            #                        str(count).zfill(2))\n",
    "            #)\n",
    "            \"\"\" \n",
    "            df_wear_resc_csv.to_csv(self.sub_dst + \"/wear_detection/wear_detection_resc_day_{}.csv\".format(\n",
    "                                    str(count).zfill(2))\n",
    "            )  \n",
    "            \"\"\"\n",
    "            all_wear_dataframes.append(df_wear_csv)\n",
    "\n",
    "        a = os.path.splitext(os.path.basename(input_file_path))\n",
    "        patient_id_csv = a[0].split('_')\n",
    "        patient_id_csv[0] \n",
    "\n",
    "        all_wear_data = pd.concat(all_wear_dataframes)\n",
    "        all_wear_data.to_csv(self.sub_dst + f\"/wear_detection/{patient_id_csv[0]}_wear_detection.csv\")\n",
    "    \n",
    "    def major_rest_period(self):\n",
    "\n",
    "        try:\n",
    "            os.mkdir(self.sub_dst + \"/major_rest_period\")  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        count = 0\n",
    "        mrps = []\n",
    "        header = [\"day\", \"major_rest_period\", \"available_hours\"]\n",
    "\n",
    "        # get days\n",
    "        days = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/raw_days/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/raw_days/\")\n",
    "                #if \".DS_Store\" not in i\n",
    "                if i.endswith(\".h5\") and \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        for day in days:\n",
    "            # DATAFRAME DF CRIADO AQUI --------\n",
    "            df = pd.read_hdf(day)\n",
    "            df = df[[\"X\", \"Y\", \"Z\", \"T\"]]\n",
    "            available_hours = (len(df) / float(self.fs)) / 3600.0 # numero de horas disponíveis de dados para o dia\n",
    "            count += 1\n",
    "\n",
    "            # process data\n",
    "            \"\"\"\n",
    "                O código aplica uma mediana móvel de 5 segundos nos dados para suavizar os sinais.\n",
    "                O ângulo do sensor é calculado usando os valores de aceleração nas direções X, Y e Z.\n",
    "                Os ângulos são reamostrados para calcular a média a cada 5 segundos. \n",
    "            \"\"\"\n",
    "\n",
    "            df = df.rolling(int(5 * self.fs)).median()  # run rolling median 5 second\n",
    "            \n",
    "            df[\"angle\"] = np.arctan(\n",
    "                df[\"Z\"] / ((df[\"X\"] ** 2 + df[\"Y\"] ** 2) ** 0.5)\n",
    "            ) * (\n",
    "                180.0 / np.pi\n",
    "            )  # get angle\n",
    "\n",
    "            df = (\n",
    "                df[[\"angle\", \"T\"]].resample(\"5s\").mean().fillna(0) # reamostrando para calcular a media de 5s\n",
    "            )  # get 5 second average\n",
    "\n",
    "            # save intermediate data for plotting\n",
    "            df[\"angle\"].to_hdf(\n",
    "                self.sub_dst\n",
    "                + \"/major_rest_period/5_second_average_arm_angle_day_{}.h5\".format(\n",
    "                    str(count).zfill(2)\n",
    "                ),\n",
    "                key=\"arm_angle_data_24hr\",\n",
    "                mode=\"w\",\n",
    "            )\n",
    "\n",
    "            \"\"\" \n",
    "                O código calcula a diferença absoluta entre o ângulo atual e o anterior. Esse valor reflete mudanças de posição.\n",
    "                Aplica uma mediana móvel de 60 minutos nos dados do ângulo e da temperatura.\n",
    "            \"\"\"\n",
    "\n",
    "            df[\"angle\"] = np.abs(\n",
    "                df[\"angle\"] - df[\"angle\"].shift(1) # indica mudança de posicao\n",
    "            )  # get absolute difference\n",
    "            df_angle = self.roll_med(df[\"angle\"], 60)  # run rolling median 5 minute\n",
    "            df_temp = self.roll_med(df[\"T\"], 60)  # run rolling median 5 minute\n",
    "\n",
    "            # calculate and apply threshold\n",
    "            thresh = np.min(\n",
    "                [\n",
    "                    np.max(\n",
    "                        [\n",
    "                            np.percentile(df_angle.Data.dropna().values, 10) * 15.0,\n",
    "                            self.minimum_rest_threshold,\n",
    "                        ]\n",
    "                    ),\n",
    "                    self.maximum_rest_threshold,\n",
    "                ]\n",
    "            )\n",
    "            df_angle.Data[df_angle.Data < thresh] = 0  # apply threshold\n",
    "            df_angle.Data[df_angle.Data >= thresh] = 1  # apply threshold\n",
    "\n",
    "            # drop rest periods where temperature is below the temp threshold\n",
    "            df_angle.Data[df_temp.Data <= self.min_t] = 1\n",
    "            df = df_angle\n",
    "\n",
    "            # BLOCOS DE DESCANSO\n",
    "            # drop rest blocks < minimum_rest_block minutes (except first and last)\n",
    "            \"\"\"     \n",
    "                Identifica os \"blocos\" de dados contínuos em df.Data. Ela faz isso calculando a \n",
    "                diferença entre os valores consecutivos em df.Data. Se a diferença não for zero (ne(0)), \n",
    "                significa que há uma transição entre um bloco de dados ativo e inativo ou vice-versa. \n",
    "                cumsum() acumula essas transições, rotulando cada bloco com um número diferente.\n",
    "            \"\"\"\n",
    "            df[\"block\"] = (df.Data.diff().ne(0)).cumsum()\n",
    "            groups, iter_count = df.groupby(by=\"block\"), 0 # agrupa com base nos grupos identificados\n",
    "            for group in groups:\n",
    "                iter_count += 1\n",
    "                if iter_count == 1 or iter_count == len(groups): # bloco inicial e final sao ignorados\n",
    "                    continue\n",
    "                if (\n",
    "                    group[1][\"Data\"].sum() == 0\n",
    "                    and len(group[1]) < 12 * self.minimum_rest_block\n",
    "                ):\n",
    "                    df.Data[group[1].index[0] : group[1].index[-1]] = 1 # descanso muito curto passa a ser atividade \n",
    "\n",
    "           # BLOCOS DE ATIVIDADE\n",
    "            # drop active blocks < allowed_rest_break minutes (except first and last)\n",
    "            df[\"block\"] = (df.Data.diff().ne(0)).cumsum()\n",
    "            groups, iter_count = df.groupby(by=\"block\"), 0\n",
    "            for group in groups:\n",
    "                iter_count += 1\n",
    "                if iter_count == 1 or iter_count == len(groups):\n",
    "                    continue\n",
    "                if (\n",
    "                    len(group[1]) == group[1][\"Data\"].sum()\n",
    "                    and len(group[1]) < 12 * self.allowed_rest_break\n",
    "                ):\n",
    "                    df.Data[group[1].index[0] : group[1].index[-1]] = 0\n",
    "\n",
    "            # get longest block\n",
    "            df[\"block\"] = (df.Data.diff().ne(0)).cumsum()\n",
    "            best = 0 # armazena o comprimento do maior periodo encontrado ate o momento\n",
    "            mrp = [] # armazena os horarios de inicio e fim do maior bloco\n",
    "            for group in df.groupby(by=\"block\"):\n",
    "                # verifica se o bloco é um período de descanso e se o comprimento do bloco atual é maior que o maior bloco de descanso encontrado até o momento\n",
    "                if group[1][\"Data\"].sum() == 0 and len(group[1]) > best:\n",
    "                    best = len(group[1])\n",
    "                    mrp = [group[1].index[0], group[1].index[-1] + pd.Timedelta(\"5m\")] # add 5 min no ultimo registro para garantir que esta completamente representado\n",
    "\n",
    "            # save predictions\n",
    "            df.drop(columns=[\"block\"], inplace=True)\n",
    "            df.to_hdf(\n",
    "                self.sub_dst\n",
    "                + \"/major_rest_period/rest_periods_day_{}.h5\".format(\n",
    "                    str(count).zfill(2)\n",
    "                ),\n",
    "                key=\"rest_period_data_24hr\",\n",
    "                mode=\"w\"\n",
    "            )\n",
    "\n",
    "            mrps.append([count, mrp, available_hours])\n",
    "\n",
    "        # aggregate and save the major rest period for each day\n",
    "        mrps = pd.DataFrame(mrps)\n",
    "        mrps.columns = header\n",
    "        mrps.set_index(\"day\", inplace=True)\n",
    "        dst = \"/major_rest_period/{}_major_rest_periods.csv\".format(self.src_name)\n",
    "        mrps.to_csv(self.sub_dst + dst)\n",
    "\n",
    "    def sleep_wake_predict(self):\n",
    "\n",
    "        try:\n",
    "            os.mkdir(\n",
    "                self.sub_dst + \"/sleep_wake_predictions\"\n",
    "            )  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        count = 0\n",
    "\n",
    "        # get days\n",
    "        days = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/activity_index_days/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/activity_index_days/\")\n",
    "                #if \".DS_Store\" not in i\n",
    "                if i.endswith(\".h5\") and \".DS_Store\" not in i  # Verifica a extensão .hdf\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        all_predictions_dataframes = []\n",
    "\n",
    "        for day in days:\n",
    "            count += 1\n",
    "            # DATAFRAME DF CRIADO -------\n",
    "            df = pd.read_hdf(day) \n",
    "\n",
    "            # run the sleep wake predictions\n",
    "            ck = ColeKripke(df.activity_index)\n",
    "            df[\"sleep_predictions\"] = ck.predict() # cria uma coluna de predicoes no df\n",
    "\n",
    "            # save predictions\n",
    "            df.drop(inplace=True, columns=[\"activity_index\"])\n",
    "            df.to_hdf(\n",
    "                self.sub_dst\n",
    "                + \"/sleep_wake_predictions/sleep_wake_day_{}.h5\".format(\n",
    "                    str(count).zfill(2)\n",
    "                ),\n",
    "                key=\"sleep_wake_data_24hr\",\n",
    "                mode=\"w\",\n",
    "            )\n",
    "\n",
    "            all_predictions_dataframes.append(df)\n",
    "        \n",
    "        a = os.path.splitext(os.path.basename(input_file_path))\n",
    "        patient_id_csv = a[0].split('_')\n",
    "        patient_id_csv[0]\n",
    "\n",
    "        all_predictions_data = pd.concat(all_predictions_dataframes)\n",
    "        all_predictions_data.to_csv(self.sub_dst + f\"/sleep_wake_predictions/{patient_id_csv[0]}_ck_predictions.csv\")\n",
    "\n",
    "    def calculate_endpoints(self):\n",
    "\n",
    "        try:\n",
    "            os.mkdir(self.sub_dst + \"/sleep_endpoints\")  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        count = 0\n",
    "\n",
    "        # get days\n",
    "        days = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/sleep_wake_predictions/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/sleep_wake_predictions/\")\n",
    "                #if \".DS_Store\" not in i\n",
    "                if i.endswith(\".h5\") and \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # get major rest periods for each day\n",
    "        mrps = pd.read_csv(\n",
    "            self.sub_dst\n",
    "            + \"/major_rest_period/{}_major_rest_periods.csv\".format(self.src_name),\n",
    "            parse_dates=True,\n",
    "            index_col=\"day\",\n",
    "        )\n",
    "        endpoints = []\n",
    "        for day in days:\n",
    "            count += 1\n",
    "            df = pd.read_hdf(day) # hdf das predicoes\n",
    "            # get and format times\n",
    "            times = mrps.loc[count].major_rest_period # extrai o MRP para o dia atual\n",
    "\n",
    "            \"\"\"    \n",
    "                Este bloco tenta formatar o período de descanso (armazenado como uma string) para \n",
    "                um formato de tempo válido para indexação do DataFrame df. Ele usa o método eval() para \n",
    "                avaliar a string e converte os tempos em valores de índice. Em seguida, seleciona os \n",
    "                dados de predições de sono e vigília apenas dentro do período de maior descanso.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                idt = times.index(\"[T\")\n",
    "                times = times[: idt + 1] + \"pd.\" + times[idt + 1 :]\n",
    "                idt = times.index(\", \")\n",
    "                times = times[: idt + 2] + \"pd.\" + times[idt + 2 :]\n",
    "                times = eval(times)\n",
    "                df = df.loc[times[0] : times[1]]\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "            # PARAMETROS IMPORTANTES PARA AVALIACAO DO SONO  -----------------------------------------\n",
    "            # get total sleep time\n",
    "            tst = len(df) - sum(df.values)\n",
    "\n",
    "            # get percent time asleep\n",
    "            pct_time_sleep = 100.0 * (len(df) - sum(df.values)) / float(len(df))\n",
    "\n",
    "            # get wake after sleep onset\n",
    "            waso = df.loc[df.idxmin()[0] :]\n",
    "            waso = waso.sum()[0]\n",
    "\n",
    "            # get sleep onset latency\n",
    "            sleep_onset_lat = (df.idxmin()[0] - df.index[0]).total_seconds() / 60.0\n",
    "\n",
    "            # number of wake bouts\n",
    "            num_wake_bouts = 0\n",
    "            wake_bout_df = df.copy()\n",
    "            wake_bout_df[\"block\"] = (\n",
    "                wake_bout_df.sleep_predictions.diff().ne(0)\n",
    "            ).cumsum()\n",
    "            for group in wake_bout_df.groupby(by=\"block\"):\n",
    "                if group[1][\"sleep_predictions\"].sum() > 0:\n",
    "                    num_wake_bouts += 1\n",
    "            endpoints.append(\n",
    "                [\n",
    "                    int(count),\n",
    "                    int(tst[0]),\n",
    "                    int(np.round(pct_time_sleep[0])),\n",
    "                    int(waso),\n",
    "                    int(sleep_onset_lat),\n",
    "                    int(num_wake_bouts),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # build and save output dataframe\n",
    "        hdr = [\n",
    "            \"day\",\n",
    "            \"total_sleep_time\",\n",
    "            \"percent_time_asleep\",\n",
    "            \"waso\",\n",
    "            \"sleep_onset_latency\",\n",
    "            \"number_wake_bouts\",\n",
    "        ]\n",
    "        endpoints = pd.DataFrame(endpoints)\n",
    "        endpoints.columns = hdr\n",
    "        endpoints.set_index(endpoints.day, inplace=True)\n",
    "        endpoints.drop(columns=\"day\", inplace=True)\n",
    "        endpoints.to_csv(self.sub_dst + \"/sleep_endpoints/sleep_endpoints_summary.csv\")\n",
    "\n",
    "    def roll_med(self, df, num_samples):\n",
    "\n",
    "        # initialize indexer and rolling median list\n",
    "        idx = 0 # percorre o df\n",
    "        med = [] # armazena medianas\n",
    "        while idx < len(df) - num_samples:\n",
    "            med.append(\n",
    "                [df.index[idx], df.iloc[idx : idx + num_samples].median()]\n",
    "            )  # get start index, std value\n",
    "            idx += 1\n",
    "        # format data frame\n",
    "        df = pd.DataFrame(med, columns=[\"Time\", \"Data\"])\n",
    "        df.set_index(df.Time, inplace=True)\n",
    "        df.drop(inplace=True, columns=\"Time\")\n",
    "        return df\n",
    "\n",
    "    def roll_std_60_minute(self, df):\n",
    "        # initialize indexer and rolling std list\n",
    "        idx = 0\n",
    "        rstd = [] # resultados do std\n",
    "\n",
    "        # calculate std for all windows\n",
    "        # 900s = 15min\n",
    "        while idx < len(df) - int(900 * self.fs):  # run until we reach the end\n",
    "            xyz = (\n",
    "                df.iloc[idx : idx + int(3600 * self.fs)].std().values\n",
    "            )  # save std in x y and z\n",
    "            rstd.append(\n",
    "                # Armazena os valores de desvio padrao para as colunas X, Y e Z\n",
    "                [df.index[idx], xyz[0], xyz[1], xyz[2]]\n",
    "            )  # get start index of window, std values\n",
    "            idx += int(900 * self.fs)  # increment indexer by 15 minutes\n",
    "\n",
    "        # format dataframe\n",
    "        df = pd.DataFrame(rstd, columns=[\"Time\", \"X\", \"Y\", \"Z\"])\n",
    "        df.set_index(df.Time, inplace=True)\n",
    "        df.drop(inplace=True, columns=\"Time\")\n",
    "        return df\n",
    "\n",
    "    def roll_max_range_60_minute(self, df):\n",
    "        # initialize indexer and rolling range list\n",
    "        idx = 0\n",
    "        rr = []\n",
    "\n",
    "        # calculate range for all windows\n",
    "        while idx < len(df) - int(900 * self.fs):  # run until we reach the end\n",
    "            xyz = (\n",
    "                df.iloc[idx : idx + int(3600 * self.fs)].max().values\n",
    "                - df.iloc[idx : idx + int(3600 * self.fs)].min().values\n",
    "            )  # save range in x y z\n",
    "            rr.append(\n",
    "                [df.index[idx], xyz[0], xyz[1], xyz[2]]\n",
    "            )  # get start index of window, range values\n",
    "            idx += int(900 * self.fs)  # increment indexer by 15 minutes\n",
    "\n",
    "        # format dataframe\n",
    "        df = pd.DataFrame(rr, columns=[\"Time\", \"X\", \"Y\", \"Z\"])\n",
    "        df.set_index(df.Time, inplace=True)\n",
    "        df.drop(inplace=True, columns=\"Time\")\n",
    "        return df\n",
    "\n",
    "    def visualize_results(self):\n",
    "        \"\"\"\n",
    "        Generates reports to visualize endpoint summary and day to day endpoint behaviors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.mkdir(self.sub_dst + \"/reports\")  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "        # raw (all 3 axes, temp, light)\n",
    "        rdays = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/raw_days/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/raw_days/\")\n",
    "                if \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        # wear (no rescoring)\n",
    "        wdays = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/wear_detection/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/wear_detection/\")\n",
    "                if \"rescored\" not in i and \".h5\" in i and \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        # wear (with rescoring)\n",
    "        wdays_re = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/wear_detection/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/wear_detection/\")\n",
    "                if \"rescored\" in i and \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        # major rest (arm angle)\n",
    "        mrdays_aa = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/major_rest_period/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/major_rest_period/\")\n",
    "                if \"angle\" in i and \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        # major rest (periods)\n",
    "        mrdays_rp = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/major_rest_period/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/major_rest_period/\")\n",
    "                if \"angle\" not in i and \"h5\" in i and \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        # activity index (full 24 hours)\n",
    "        aidays = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/activity_index_days/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/activity_index_days/\")\n",
    "                if \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        # sleep wake (full 24 hours)\n",
    "        swdays = sorted(\n",
    "            [\n",
    "                self.sub_dst + \"/sleep_wake_predictions/\" + i\n",
    "                for i in os.listdir(self.sub_dst + \"/sleep_wake_predictions/\")\n",
    "                if \".DS_Store\" not in i\n",
    "            ]\n",
    "        )\n",
    "        # endpoints (graphs/charts per day)\n",
    "        endpoints = pd.read_csv(\n",
    "            self.sub_dst + \"/sleep_endpoints/sleep_endpoints_summary.csv\",\n",
    "            index_col=\"day\",\n",
    "        )\n",
    "\n",
    "        days = range(0, len(rdays))\n",
    "        for day in days:\n",
    "            # read the raw data, downsample for plotting\n",
    "            raw = pd.read_hdf(rdays[day])\n",
    "            raw = raw.resample(\"60s\").median()\n",
    "\n",
    "            # get shared index\n",
    "            idx = pd.date_range(\n",
    "                start=raw.index[0].replace(hour=12, minute=0, second=0, microsecond=0),\n",
    "                periods=1440,\n",
    "                freq=\"60s\",\n",
    "            )\n",
    "            raw = raw.reindex(idx, fill_value=float(\"nan\"))\n",
    "\n",
    "            # read the wear data, resample and match index with the raw data\n",
    "            wear = pd.read_hdf(wdays[day])  # 15 minute period\n",
    "            wear[wear == 0] = float(\"nan\")\n",
    "            wear = wear.resample(\"60s\").ffill()\n",
    "            wear = wear.reindex(idx, fill_value=float(\"nan\"))\n",
    "\n",
    "            # read the wear data with rescoring, resample and match the raw index\n",
    "            wear_re = pd.read_hdf(wdays_re[day])  # 15 minute period\n",
    "            wear_re[wear_re == 0] = float(\"nan\")\n",
    "            wear_re = wear_re.resample(\"60s\").ffill()\n",
    "            wear_re = wear_re.reindex(idx, fill_value=float(\"nan\"))\n",
    "\n",
    "            # read the arm angle data, resample and match the raw index\n",
    "            angle = pd.read_hdf(mrdays_aa[day])  # 5 second period\n",
    "            angle = angle.resample(\"60s\").max()\n",
    "            angle = angle.reindex(idx, fill_value=float(\"nan\"))\n",
    "\n",
    "            # read the major rest period data, resample and match the raw index\n",
    "            periods = pd.read_hdf(mrdays_rp[day])  # 5 second period\n",
    "            periods[periods == 1] = float(\"nan\")\n",
    "            periods[periods == 0] = 1\n",
    "            periods = periods.resample(\"60s\").max()\n",
    "            periods = periods.reindex(idx, fill_value=float(\"nan\"))\n",
    "\n",
    "            # read the acvitity index data, resample and match the raw index\n",
    "            aindex = pd.read_hdf(aidays[day])  # 1 minute period\n",
    "            aindex = aindex.resample(\"60s\").max()\n",
    "            aindex = aindex.reindex(idx, fill_value=float(\"nan\"))\n",
    "\n",
    "            # read the sleep wake predictions, resample and match the raw index\n",
    "            swake = pd.read_hdf(swdays[day])  # 1 minute period\n",
    "            swake[swake == 0] = float(\"nan\")\n",
    "            swake = swake.resample(\"60s\").max()\n",
    "            swake = swake.reindex(idx, fill_value=float(\"nan\"))\n",
    "\n",
    "            # build a dataframe for plotting certain data streams as straight lines\n",
    "            df = swake.copy()\n",
    "            df.columns = [\"wake\"]\n",
    "            df[\"rest periods\"] = periods.values - 0.05\n",
    "            df[\"on body\"] = wear.values - 0.1\n",
    "            df[\"on body(rescore)\"] = wear_re.values - 0.15\n",
    "            swake, wear, wear_re, periods = [], [], [], []\n",
    "\n",
    "            # get day endpoints for plotting of table\n",
    "            t_labels = (\n",
    "                \"Total Sleep Time(minutes)\",\n",
    "                \"Percent Time Asleep\",\n",
    "                \"Wake After Sleep Onset(minutes)\",\n",
    "                \"Sleep Onset Latency(minutes)\",\n",
    "                \"Number of Wake Bouts\",\n",
    "            )\n",
    "            t_vals = [endpoints.loc[day + 1].values]\n",
    "\n",
    "            # plotting\n",
    "            fig, (axt, ax0, ax1, ax2, ax3, ax4, ax5) = plt.subplots(\n",
    "                7, 1, figsize=(30, 15)\n",
    "            )\n",
    "            plt.suptitle(\n",
    "                \"Visual Report for Source: {}\\nDay: {}\\nDate: {}\".format(\n",
    "                    self.src_name, day + 1, idx[0].date()\n",
    "                ),\n",
    "                fontsize=25,\n",
    "            )\n",
    "            hours = mdates.HourLocator(interval=1)\n",
    "            h_fmt = mdates.DateFormatter(\"%H:%M\")\n",
    "            all_axes = (ax0, ax1, ax2, ax3, ax4, ax5)\n",
    "\n",
    "            # plot table\n",
    "            tbl = axt.table(\n",
    "                cellText=t_vals,\n",
    "                colLabels=t_labels,\n",
    "                cellLoc=\"center\",\n",
    "                rowLoc=\"center\",\n",
    "                loc=\"center\",\n",
    "                fontsize=20,\n",
    "            )\n",
    "            tbl.auto_set_font_size(False)\n",
    "            tbl.set_fontsize(24)\n",
    "            tbl.scale(1.1, 2.4)\n",
    "            axt.axis(\"off\")\n",
    "\n",
    "            # plot raw\n",
    "            raw.rename(columns={\"T\": \"Temperature\", \"LUX\": \"Light\"}, inplace=True)\n",
    "            raw[[\"X\", \"Y\", \"Z\"]].plot(ax=ax0, lw=1).legend(\n",
    "                bbox_to_anchor=(0, 1), fontsize=20\n",
    "            )\n",
    "            ax0.set_ylabel(\"\")\n",
    "            ax0.set_xlabel(\"\")\n",
    "\n",
    "            # plot temperature\n",
    "            raw[[\"Temperature\"]].plot(\n",
    "                ax=ax1, lw=1, color=sns.xkcd_rgb[\"pale red\"]\n",
    "            ).legend(bbox_to_anchor=(0, 1), fontsize=20)\n",
    "            ax1.axhline(y=self.min_t, color=\"r\", linestyle=\"--\", lw=2)\n",
    "            props = dict(boxstyle=\"round\", facecolor=\"lavender\", alpha=0.35)\n",
    "            textstr = u\"max: {}\\xb0C\\nmin: {}\\xb0C\\nthresh: {}\\xb0C\".format(\n",
    "                raw[[\"Temperature\"]].max().values[0],\n",
    "                raw[[\"Temperature\"]].min().values[0],\n",
    "                self.min_t,\n",
    "            )\n",
    "            ax1.text(\n",
    "                0.005,\n",
    "                0.95,\n",
    "                textstr,\n",
    "                transform=ax1.transAxes,\n",
    "                fontsize=14,\n",
    "                verticalalignment=\"top\",\n",
    "                bbox=props,\n",
    "            )\n",
    "            ax1.set_ylabel(\"\")\n",
    "            ax1.set_xlabel(\"\")\n",
    "\n",
    "            # plot light\n",
    "            raw[[\"Light\"]].plot(ax=ax2, lw=1, color=sns.xkcd_rgb[\"pale orange\"]).legend(\n",
    "                bbox_to_anchor=(0, 1), fontsize=20\n",
    "            )\n",
    "            ax2.set_ylabel(\"\")\n",
    "            ax2.set_xlabel(\"\")\n",
    "\n",
    "            # plot activity index\n",
    "            aindex.plot(ax=ax3, lw=1, color=\"#6fc276\").legend(\n",
    "                labels=[\"activity\"], bbox_to_anchor=(0, 0.75), fontsize=20\n",
    "            )\n",
    "            ax3.set_ylabel(\"\")\n",
    "            ax3.set_xlabel(\"\")\n",
    "\n",
    "            # plot arm angle\n",
    "            angle.plot(ax=ax4, lw=1, color=\"#b36ff6\").legend(\n",
    "                labels=[\"arm angle\"], bbox_to_anchor=(0, 0.75), fontsize=20\n",
    "            )\n",
    "            ax4.set_ylabel(\"\")\n",
    "            ax4.set_xlabel(\"\")\n",
    "\n",
    "            # plot dataframe of 4 streams\n",
    "            df.plot(ax=ax5, lw=8, x_compat=True).legend(\n",
    "                bbox_to_anchor=(0, 1.3), fontsize=20\n",
    "            )\n",
    "            ax5.set_ylabel(\"\")\n",
    "            ax5.set_xlabel(\"\")\n",
    "\n",
    "            # plot formatting\n",
    "            plt.draw()\n",
    "            count = 0\n",
    "            for ax in all_axes:\n",
    "                count += 1\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.spines[\"bottom\"].set_visible(False)\n",
    "                ax.spines[\"left\"].set_visible(False)\n",
    "                ax.grid(False)\n",
    "                if count < 6:\n",
    "                    ax.get_xaxis().set_ticks([])\n",
    "                ax.get_yaxis().set_ticks([])\n",
    "            ax5.xaxis.set_major_locator(hours)\n",
    "            ax5.xaxis.set_major_formatter(h_fmt)\n",
    "            plt.subplots_adjust(wspace=0, hspace=0)\n",
    "            fig.autofmt_xdate()\n",
    "            for tick in ax5.xaxis.get_major_ticks():\n",
    "                #tick.label.set_fontsize(16)\n",
    "                plt.xticks(fontsize=16)\n",
    "            plt.savefig(\n",
    "                self.sub_dst + \"/reports/Visual_Results_Day_{}.pdf\".format(day + 1)\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "        # generate a summary plot from endpoint data\n",
    "        fig, (ax0, ax1, ax2, ax3, ax4) = plt.subplots(5, 1, figsize=(12, 12))\n",
    "        plt.suptitle(\"Summary Report for Source: {}\".format(self.src_name), fontsize=16)\n",
    "        all_axes = (ax0, ax1, ax2, ax3, ax4)\n",
    "        ylabels = [\n",
    "            \"Total Sleep\\nTime(min)\\nMean: {}\".format(\n",
    "                int(np.round(endpoints.total_sleep_time.mean()))\n",
    "            ),\n",
    "            \"Percent Time\\nAsleep\\nMean: {}\".format(\n",
    "                int(np.round(endpoints.percent_time_asleep.mean()))\n",
    "            ),\n",
    "            \"Wake After\\nSleep Onset(min)\\nMean: {}\".format(\n",
    "                int(np.round(endpoints.waso.mean()))\n",
    "            ),\n",
    "            \"Sleep Onset\\nLatency(min)\\nMean: {}\".format(\n",
    "                int(np.round(endpoints.sleep_onset_latency.mean()))\n",
    "            ),\n",
    "            \"Number of\\nWake Bouts\\nMean: {}\".format(\n",
    "                int(np.round(endpoints.number_wake_bouts.mean()))\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # plot total sleep time\n",
    "        endpoints.total_sleep_time.plot.bar(ax=ax0, title=\"\")\n",
    "        # plot percent time asleep\n",
    "        endpoints.percent_time_asleep.plot.bar(ax=ax1, title=\"\")\n",
    "        # plot wake after sleep onset\n",
    "        endpoints.waso.plot.bar(ax=ax2, title=\"\")\n",
    "        # plot sleep onset latency\n",
    "        endpoints.sleep_onset_latency.plot.bar(ax=ax3, title=\"\")\n",
    "        # plot the number of wake bouts\n",
    "        endpoints.number_wake_bouts.plot.bar(ax=ax4, title=\"\")\n",
    "        # plot formatting\n",
    "        count = 0\n",
    "        for ax in all_axes:\n",
    "            count += 1\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "            ax.spines[\"bottom\"].set_visible(False)\n",
    "            ax.spines[\"left\"].set_visible(False)\n",
    "            ax.grid(False)\n",
    "            ax.set_ylabel(ylabels[count - 1], rotation=0, fontsize=12, labelpad=50)\n",
    "            if count < 5:\n",
    "                ax.set_xlabel(\"\")\n",
    "                ax.get_xaxis().set_ticks([])\n",
    "                ax.get_yaxis().set_ticks([])\n",
    "            else:\n",
    "                ax.set_xlabel(\"Day\", fontsize=20)\n",
    "                ax.get_yaxis().set_ticks([])\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(\n",
    "                    np.round(p.get_height(), decimals=2),\n",
    "                    (p.get_x() + p.get_width() / 2.0, 0),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    xytext=(0, 10),\n",
    "                    textcoords=\"offset points\",\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "        plt.subplots_adjust(wspace=0, hspace=0.01)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.draw()\n",
    "        plt.savefig(self.sub_dst + \"/reports/Summary_Report.pdf\")\n",
    "        plt.close()\n",
    "\n",
    "    def rescore(self, df):\n",
    "\n",
    "        # group classifications into wear and nonwear blocks\n",
    "\n",
    "        \"\"\"      \n",
    "            df[\"block\"]: Cria uma coluna que identifica blocos de períodos consecutivos de uso (wear) ou \n",
    "            não uso (nonwear). A função diff().ne(0) detecta mudanças nos valores de uso e cumsum() cria um \n",
    "            contador cumulativo para distinguir blocos.\n",
    "        \"\"\"\n",
    "        df[\"block\"] = (df.wear.diff().ne(0)).cumsum()\n",
    "        blocks = list(df.groupby(\"block\"))\n",
    "\n",
    "        # iterate through blocks\n",
    "        for i in range(1, len(blocks) - 1):\n",
    "            wear = blocks[i][1][\"wear\"].values[\n",
    "                0\n",
    "            ]  # get whether or not the block is wear\n",
    "            if wear:\n",
    "                # get hour lengths of the previous, current, and next blocks\n",
    "                #calculo da duracao dos bloocos adjacentes\n",
    "                prev, current, post = (\n",
    "                    len(blocks[i - 1][1]) * 0.25, # multiplicado por 0.25 para representar os 15 min\n",
    "                    len(blocks[i][1]) * 0.25,\n",
    "                    len(blocks[i + 1][1]) * 0.25,\n",
    "                )\n",
    "                # if the current block is less than 3 hours and the ratio to previous and post blocks is less than 80%\n",
    "                if current < 3 and current / (prev + post) < 0.8:\n",
    "                    df[\"wear\"][\n",
    "                        df.block == blocks[i][0]\n",
    "                    ] = 0  # rescore the wear period as non wear\n",
    "                # if the current block is less than 6 hours and the ratio to previous and post blocks is less than 30%\n",
    "                elif current < 6 and current / (prev + post) < 0.3:\n",
    "                    df[\"wear\"][\n",
    "                        df.block == blocks[i][0]\n",
    "                    ] = 0  # rescore the wear period as non wear\n",
    "        df.drop(columns=[\"block\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def rescore_last_day(self, df):\n",
    "        # group classifications into wear and nonwear blocks\n",
    "        df[\"block\"] = (df.wear.diff().ne(0)).cumsum()\n",
    "        blocks = list(df.groupby(\"block\"))\n",
    "\n",
    "        # get the start index of the last day\n",
    "        last_day_index = df.index[-1] - pd.to_timedelta(\"24h\")\n",
    "\n",
    "        # iterate through blocks\n",
    "        for i in range(1, len(blocks)):\n",
    "            wear = blocks[i][1][\"wear\"].values[\n",
    "                0\n",
    "            ]  # get whether or not the block is wear\n",
    "            if (\n",
    "                wear and blocks[i][1].index[0] > last_day_index\n",
    "            ):  # if wear, and it's the last day\n",
    "                # get hour lengths of the previous and current blocks\n",
    "                prev, current = len(blocks[i - 1][1]) * 0.25, len(blocks[i][1]) * 0.25\n",
    "                # if the current block is less than 3 hours and the previous block is greater or equal to 1 hour\n",
    "                if current < 3 and prev >= 1:\n",
    "                    df[\"wear\"][\n",
    "                        df.block == blocks[i][0]\n",
    "                    ] = 0  # rescore the wear period as non wear\n",
    "        df.drop(columns=[\"block\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def aggregate_results(self):\n",
    "        \"\"\"\n",
    "        Aggregates all results in a single folder.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.mkdir(self.sub_dst + \"/results\")  # set up output directory\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # collect results files\n",
    "        srcs = []\n",
    "        srcs += [\n",
    "            self.sub_dst + \"/reports/\" + x\n",
    "            for x in os.listdir(self.sub_dst + \"/reports\")\n",
    "            if \".DS_Store\" not in x\n",
    "        ]\n",
    "        srcs += [\n",
    "            self.sub_dst + \"/major_rest_period/\" + x\n",
    "            for x in os.listdir(self.sub_dst + \"/major_rest_period\")\n",
    "            if \".csv\" in x and \".DS_Store\" not in x\n",
    "        ]\n",
    "        srcs += [\n",
    "            self.sub_dst + \"/sleep_endpoints/\" + x\n",
    "            for x in os.listdir(self.sub_dst + \"/sleep_endpoints\")\n",
    "            if \".DS_Store\" not in x\n",
    "        ]\n",
    "\n",
    "        # aggregate\n",
    "        for src in srcs:\n",
    "            copy(src, self.sub_dst + \"/results\")\n",
    "\n",
    "    def clear_data(self):\n",
    "        \"\"\"\n",
    "        Clears all intermediate data, keeping only results.\n",
    "\n",
    "        \"\"\"\n",
    "        # collect directories for deletion\n",
    "        direcs = [\n",
    "            os.path.join(self.sub_dst, x)\n",
    "            for x in os.listdir(self.sub_dst)\n",
    "            if \"results\" not in x and \".DS_Store\" not in x\n",
    "        ]\n",
    "\n",
    "        # delete\n",
    "        for direc in direcs:\n",
    "            rmtree(direc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seguintes funções são globais, utilizadas por mais de uma classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def band_pass_filter(\n",
    "    data_df, sampling_rate, bp_cutoff, order, channels=[\"X\", \"Y\", \"Z\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Band-pass filter a given sensor signal.\n",
    "\n",
    "    :param data_df: dataframe housing sensor signals\n",
    "    :param sampling_rate: sampling rate of signal\n",
    "    :param bp_cutoff: filter cutoffs\n",
    "    :param order: filter order\n",
    "    :param channels: channels of signal to filter\n",
    "    :return: dataframe of raw and filtered data\n",
    "    \"\"\"\n",
    "    data = data_df[channels].values\n",
    "\n",
    "    # Calculate the critical frequency (radians/sample) based on cutoff frequency (Hz) and sampling rate (Hz)\n",
    "    critical_frequency = [\n",
    "        bp_cutoff[0] * 2.0 / sampling_rate,\n",
    "        bp_cutoff[1] * 2.0 / sampling_rate,\n",
    "    ]\n",
    "\n",
    "    # Get the numerator (b) and denominator (a) of the IIR filter\n",
    "    [b, a] = signal.butter(\n",
    "        N=order, Wn=critical_frequency, btype=\"bandpass\", analog=False\n",
    "    )\n",
    "\n",
    "    # Apply filter to raw data\n",
    "    bp_filtered_data = signal.filtfilt(b, a, data, padlen=10, axis=0)\n",
    "\n",
    "    new_channel_labels = [ax + \"_bp_filt_\" + str(bp_cutoff) for ax in channels]\n",
    "\n",
    "    data_df[new_channel_labels] = pd.DataFrame(bp_filtered_data)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def activity_index(signal_df, channels=[\"X\", \"Y\", \"Z\"]):\n",
    "    \"\"\"\n",
    "    Compute activity index of sensor signals.\n",
    "\n",
    "    :param signal_df: dataframe housing desired sensor signals\n",
    "    :param channels: channels of signal to compute activity index\n",
    "    :return: dataframe housing calculated activity index\n",
    "    \"\"\"\n",
    "    ai_df = pd.DataFrame()\n",
    "\n",
    "    \"\"\"\n",
    "        np.var(signal_df[channels], axis=0): Calcula a variância de cada um dos canais fornecidos, \n",
    "        ao longo das linhas do DataFrame (axis=0 significa que a variância é calculada ao longo de cada coluna).\n",
    "        .mean() ** 0.5 = desvio padrao\n",
    "\n",
    "    \"\"\"\n",
    "    ai_df[\"activity_index\"] = [np.var(signal_df[channels], axis=0).mean() ** 0.5]\n",
    "    return ai_df # armazena o valor do índice de atividade\n",
    "\n",
    "\n",
    "def bin2df(full_path):\n",
    "    \"\"\"\n",
    "\n",
    "    Reads geneactiv .bin files into a pandas dataframe\n",
    "\n",
    "    :param full_path: full path to geneactiv .bin file\n",
    "\n",
    "    :return decode: pandas dataframe of GA data\n",
    "\n",
    "    \"\"\"\n",
    "    with open(full_path, \"rb\") as in_file:\n",
    "        full_line = in_file.readline() # le a primeira linha \n",
    "        count = 0 # contador de linhas lidas\n",
    "        fs = \"\" # frequencia de amostragem\n",
    "        df = [] # armazena dfs intermediarios\n",
    "        while full_line:\n",
    "            full_line = in_file.readline()\n",
    "            line = full_line[:].split(\"\\r\\n\")[0] # remove quebra de linha\n",
    "            count += 1\n",
    "            if count < 60: # parametros de calibracao\n",
    "                if \"x gain\" in line:\n",
    "                    x_gain = int(line.split(\":\")[-1])\n",
    "\n",
    "                if \"x offset\" in line:\n",
    "                    x_offset = int(line.split(\":\")[-1])\n",
    "\n",
    "                if \"y gain\" in line:\n",
    "                    y_gain = int(line.split(\":\")[-1])\n",
    "\n",
    "                if \"y offset\" in line:\n",
    "                    y_offset = int(line.split(\":\")[-1])\n",
    "\n",
    "                if \"z gain\" in line:\n",
    "                    z_gain = int(line.split(\":\")[-1])\n",
    "\n",
    "                if \"z offset\" in line:\n",
    "                    z_offset = int(line.split(\":\")[-1])\n",
    "\n",
    "                if \"Volts\" in line:\n",
    "                    volts = int(line.split(\":\")[-1])\n",
    "\n",
    "                if \"Lux\" in line:\n",
    "                    lux = int(line.split(\":\")[-1])\n",
    "\n",
    "            if \"Page Time:\" in line: # processamento do tempo\n",
    "                time = pd.to_datetime(\n",
    "                    \":\".join(line.split(\":\")[1:])[0:-2], format=\"%Y-%m-%d %H:%M:%S:%f\"\n",
    "                )\n",
    "\n",
    "            if \"Temperature:\" in line:\n",
    "                temp = float(line.split(\":\")[-1])\n",
    "\n",
    "            if not fs:\n",
    "                if \"Measurement Frequency:\" in line: # leitura da frequencia\n",
    "                    fs = float(line.split(\":\")[-1].split(\" \")[0])\n",
    "                    offset = np.array([1 / fs] * 300) * np.arange(0, 300)\n",
    "                    delta = pd.to_timedelta(offset, unit=\"s\")\n",
    "\n",
    "            \"\"\" \n",
    "                Quando uma linha contém 3600 caracteres (indicação de blocos de dados), o método decodifica \n",
    "                os valores hexadecimais para binário, e os converte em valores para os eixos X, Y, Z, luminosidade\n",
    "                e outros.\n",
    "            \"\"\"\n",
    "            \n",
    "            if len(line) == 3600:\n",
    "                # hex to bin\n",
    "                hexes = struct.unpack(\"12s \" * 300, line)\n",
    "                bins = (\n",
    "                    struct.unpack(\n",
    "                        \"12s 12s 12s 10s 1s 1s\", bin(int(hx, 16))[2:].zfill(48)\n",
    "                    )\n",
    "                    for hx in hexes\n",
    "                )\n",
    "                decode = pd.DataFrame(\n",
    "                    bins,\n",
    "                    columns=[\"X\", \"Y\", \"Z\", \"LUX\", \"Button\", \"_\"],\n",
    "                    index=pd.DatetimeIndex([time] * 300) + delta,\n",
    "                )\n",
    "\n",
    "                # binary to decimal and calibration\n",
    "                decode.X = decode.X.apply(\n",
    "                    lambda x: round(\n",
    "                        (BitArray(bin=x).int * 100.0 - x_offset) / x_gain, 4\n",
    "                    )\n",
    "                )\n",
    "                decode.Y = decode.Y.apply(\n",
    "                    lambda x: round(\n",
    "                        (BitArray(bin=x).int * 100.0 - y_offset) / y_gain, 4\n",
    "                    )\n",
    "                )\n",
    "                decode.Z = decode.Z.apply(\n",
    "                    lambda x: round(\n",
    "                        (BitArray(bin=x).int * 100.0 - z_offset) / z_gain, 4\n",
    "                    )\n",
    "                )\n",
    "                decode.LUX = decode.LUX.apply(lambda x: int(x, 2) * lux / volts)\n",
    "                decode[\"T\"] = temp\n",
    "                df.append(decode)\n",
    "\n",
    "        df = pd.concat(df, axis=0)\n",
    "        df.index.name = \"Time\"\n",
    "        return df[[\"X\", \"Y\", \"Z\", \"LUX\", \"T\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementação da classe ColeKripke para utilização do algoritmo de mesmo nome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColeKripke:\n",
    "    \"\"\"\n",
    "    Runs sleep wake detection on epoch level activity data. Epochs are 1 minute long and activity is represented\n",
    "    by an activity index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activity_index):\n",
    "        \"\"\"\n",
    "        Initialization of the class\n",
    "\n",
    "        :param activity_index: pandas dataframe of epoch level activity index values\n",
    "        \"\"\"\n",
    "        self.activity_index = activity_index\n",
    "        self.predictions = None\n",
    "\n",
    "    def predict(self, sf=np.array(0.193125)):\n",
    "        \"\"\"\n",
    "        Runs the prediction of sleep wake states based on activity index data.\n",
    "\n",
    "        :param sf: scale factor to use for the predictions (default corresponds to scale factor optimized for use with\n",
    "        the activity index, if other activity measures are desired the scale factor can be modified or optimized.)\n",
    "        The recommended range for the scale factor is between 0.1 and 0.25 depending on the sensitivity to activity\n",
    "        desired, and possibly the population being observed.\n",
    "\n",
    "        :return: rescored predictions\n",
    "        \"\"\"\n",
    "        kernel = (\n",
    "            sf\n",
    "            * np.array([4.64, 6.87, 3.75, 5.07, 16.19, 5.84, 4.024, 0.00, 0.00])[::-1]\n",
    "        )\n",
    "        scores = np.convolve(self.activity_index, kernel, \"same\")\n",
    "        scores[scores >= 0.5] = 1\n",
    "        scores[scores < 0.5] = 0\n",
    "\n",
    "        # rescore the original predictions\n",
    "        self.rescore(scores)\n",
    "        return self.predictions\n",
    "\n",
    "    def rescore(self, predictions):\n",
    "        \"\"\"\n",
    "        Application of Webster's rescoring rules as described in the Cole-Kripke paper.\n",
    "\n",
    "        :param predictions: array of predictions\n",
    "        :return: rescored predictions\n",
    "        \"\"\"\n",
    "        rescored = predictions.copy()\n",
    "        # rules a through c\n",
    "        wake_bin = 0\n",
    "        for t in range(len(rescored)):\n",
    "            if rescored[t] == 1:\n",
    "                wake_bin += 1\n",
    "            else:\n",
    "                if (\n",
    "                    14 < wake_bin\n",
    "                ):  # rule c: at least 15 minutes of wake, next 4 minutes of sleep get rescored\n",
    "                    rescored[t : t + 4] = 1.0\n",
    "                elif (\n",
    "                    9 < wake_bin < 15\n",
    "                ):  # rule b: at least 10 minutes of wake, next 3 minutes of sleep get rescored\n",
    "                    rescored[t : t + 3] = 1.0\n",
    "                elif (\n",
    "                    3 < wake_bin < 10\n",
    "                ):  # rule a: at least 4 minutes of wake, next 1 minute of sleep gets rescored\n",
    "                    rescored[t] = 1.0\n",
    "                wake_bin = 0\n",
    "        # rule d: 6 minutes or less of sleep surrounded by at least 10 minutes of wake on each side gets rescored\n",
    "        sleep_bin = 0\n",
    "        start_ind = 0\n",
    "        for t in range(10, len(rescored) - 10):\n",
    "            if rescored[t] == 0:\n",
    "                sleep_bin += 1\n",
    "                if sleep_bin == 1:\n",
    "                    start_ind = t\n",
    "            else:\n",
    "                if 0 < sleep_bin <= 6:\n",
    "                    if (\n",
    "                        sum(rescored[start_ind - 10 : start_ind]) == 10.0\n",
    "                        and sum(rescored[t : t + 10]) == 10.0\n",
    "                    ):\n",
    "                        rescored[start_ind:t] = 1.0\n",
    "                sleep_bin = 0\n",
    "        self.predictions = rescored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atribuindo os valores para os atributos da classe SleepPY.\n",
    "\n",
    "\n",
    "<font color=blue> Descrição das variáveis informadas pelo usuário: </font>\n",
    "- input_file: caminho completo para o arquivo a ser processado  \n",
    "            \n",
    "        Ex: 'BIN/1379-38_left wrist_046456_2018-08-03 10-23-38.csv'\n",
    "- results_directory = caminho completo para o diretório onde os resultados devem ser salvos\n",
    "- sampling_frequency = frequência de amostragem em que os dados foram gravados\n",
    "- start_buffer: número de segundos a serem ignorados no início da gravação \n",
    "- stop_buffer: número de segundos a serem ignorados no final da gravação\n",
    "- start_time: data e hora a partir da qual o processamento dos dados deve começar (string) no formato: \"%Y-%m-%d %H:%M:%S:%f\"\n",
    "- stop_time: data e hora em que o processamento dos dados deve parar (string) no formato: \"%Y-%m-%d %H:%M:%S:%f\"\n",
    "- run_config: valor inteiro entre 0 e 6, que define qual etapa do processamento será executada \n",
    "\n",
    "        - self.run_config <= 0: split the data into 24 hour periods  \n",
    "        - self.run_config <= 1: extract the activity index feature \n",
    "        - self.run_config <= 2: run wear/on-body detection  \n",
    "        - self.run_config <= 3: run major rest period detection  \n",
    "        - self.run_config <= 4: run sleep wake predictions on the major rest period  \n",
    "        - self.run_config <= 5: calculate endpoints based on the above predictions  \n",
    "        - self.run_config <= 6: generates visual reports\n",
    "        \n",
    "- temperature_threshold: temperatura mínima aceitável para considerar um período de repouso como candidato válido\n",
    "- minimum_rest_block: número mínimo de minutos necessário para considerar um período de repouso válido (inteiro)\n",
    "- allowed_rest_break: número de minutos permitidos para interromper um período de repouso maior (inteiro)\n",
    "- minimum_rest_threshold: limite mínimo permitido para determinar um período de repouso principal (valor decimal)\n",
    "- maximum_rest_threshold: limite máximo permitido para determinar um período de repouso principal (valor decimal)\n",
    "- minimum_hours: número mínimo de horas necessárias para considerar um dia como utilizável (inteiro)\n",
    "- clear_intermediate_data: indicador booleano para limpar todos os dados intermediários\n",
    "- aws_object: objeto de dados a ser processado a partir do AWS (em substituição ao caminho do arquivo de origem)\n",
    "- verbose: booleano que define se o status deve ser impresso durante o processamento\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usuário deve informar\n",
    "\n",
    "folder_path = \"C:/Users/marim/Documents/Faculdade/TCC/patient_act_data_CSV\"\n",
    "sleep_sum_path = 'C:/Users/marim/Documents/Faculdade/TCC/HEATMAP/sleep_sum_TOTAL.csv'\n",
    "folder_name = \"patient_act_data_CSV\"\n",
    "key = \"Measurement Frequency\"\n",
    "results_directory = 'C:/Users/marim/Documents/Faculdade/TCC/' + folder_name\n",
    "# run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatizando a leitura de arquivos\n",
    "\n",
    "import csv_format as cs\n",
    "from pathlib import Path\n",
    "\n",
    "# Lendo os arquivos CSV 1379-38_left wrist_046456_2018-08-03 10-23-38\n",
    "# patient_act_data -> pasta com os csv extraidos no app do GENEActiv\n",
    "# Itera sobre todos os arquivos na pasta\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):  # Verifica se o arquivo é CSV\n",
    "        #input_file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        input_file_path = Path(folder_path) / filename\n",
    "        input_file_path = str(input_file_path).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        var = cs.split_100_csv(input_file_path, key)\n",
    "\n",
    "        if key == \"Measurement Frequency\": #colocar como opcao?\n",
    "            var = int(var)\n",
    "            sampling_frequency = var\n",
    "\n",
    "        input_file = input_file_path\n",
    "        #results_directory = 'C:/Users/marim/Documents/Faculdade/TCC/' + folder_path\n",
    "        #sampling_frequency = 50\n",
    "        start_buffer=\"0s\"\n",
    "        stop_buffer=\"0s\"\n",
    "        start_time=\"\"\n",
    "        stop_time=\"\"\n",
    "        run_config=0\n",
    "        temperature_threshold=25.0\n",
    "        minimum_rest_block=30\n",
    "        allowed_rest_break=60\n",
    "        minimum_rest_threshold=0.0\n",
    "        maximum_rest_threshold=1000.0\n",
    "        minimum_hours=6\n",
    "        clear_intermediate_data=False\n",
    "        aws_object=None\n",
    "        verbose=False\n",
    "\n",
    "        print(input_file_path) # esta lendo certo \n",
    "        # aqui pode chamar a funcao sleeppy\n",
    "        # ver onde os resultados vao ser salvos \n",
    "        print(results_directory)\n",
    "\n",
    "        print(sampling_frequency)\n",
    "        \n",
    "        patient = SleepPy(input_file, \n",
    "                results_directory, \n",
    "                sampling_frequency, \n",
    "                start_buffer, \n",
    "                stop_buffer, \n",
    "                start_time, \n",
    "                stop_time, \n",
    "                run_config, \n",
    "                temperature_threshold, \n",
    "                minimum_rest_block, \n",
    "                allowed_rest_break, \n",
    "                minimum_rest_threshold, \n",
    "                maximum_rest_threshold, \n",
    "                minimum_hours, \n",
    "                clear_intermediate_data, \n",
    "                aws_object, \n",
    "                verbose\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import merge_data as mgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo arquivo activity: C:/Users/marim/Documents/Faculdade/TCC/patient_act_data_CSV\\1379-38_left wrist_046456_2018-08-03 10-23-38\\1379-38_activity_index_day.csv\n",
      "Lendo arquivo activity: C:/Users/marim/Documents/Faculdade/TCC/patient_act_data_CSV\\1379-38_left wrist_046456_2018-08-03 10-23-38\\1379-38_ck_predictions.csv\n",
      "Lendo arquivo raw_data: C:/Users/marim/Documents/Faculdade/TCC/patient_act_data_CSV\\1379-38_left wrist_046456_2018-08-03 10-23-38\\1379-38_raw_data.csv\n",
      "Lendo arquivo activity: C:/Users/marim/Documents/Faculdade/TCC/patient_act_data_CSV\\1379-38_left wrist_046456_2018-08-03 10-23-38\\1379-38_wear_detection.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marim\\Documents\\Faculdade\\TCC\\sleep_package\\merge_data.py:88: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  sleep_sum_cut.calendar_date = pd.to_datetime(sleep_sum_cut.calendar_date)\n",
      "c:\\Users\\marim\\Documents\\Faculdade\\TCC\\sleep_package\\merge_data.py:112: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df['activity_index'] = merged_df['activity_index'].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tabela: 1379-38\n",
      "         Date      Time       X       Y       Z     T  Sleep  activity_index\n",
      "0  2018-07-27  15:55:55 -0.0359 -0.2505 -1.0169  36.2      0        0.157358\n",
      "1  2018-07-27  15:55:55 -0.1655 -0.1098 -0.8518  36.2      0        0.157358\n",
      "2  2018-07-27  15:55:55 -0.1026 -0.1528 -0.9203  36.2      0        0.157358\n",
      "3  2018-07-27  15:55:55 -0.0084 -0.1958 -0.9806  36.2      0        0.157358\n",
      "4  2018-07-27  15:55:55 -0.0045 -0.2154 -0.9243  36.2      0        0.157358\n"
     ]
    }
   ],
   "source": [
    "mgd.read_csv_from_subdirs(folder_path, sleep_sum_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>T</th>\n",
       "      <th>Sleep</th>\n",
       "      <th>activity_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>15:55:55</td>\n",
       "      <td>-0.0359</td>\n",
       "      <td>-0.2505</td>\n",
       "      <td>-1.0169</td>\n",
       "      <td>36.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>15:55:55</td>\n",
       "      <td>-0.1655</td>\n",
       "      <td>-0.1098</td>\n",
       "      <td>-0.8518</td>\n",
       "      <td>36.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>15:55:55</td>\n",
       "      <td>-0.1026</td>\n",
       "      <td>-0.1528</td>\n",
       "      <td>-0.9203</td>\n",
       "      <td>36.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>15:55:55</td>\n",
       "      <td>-0.0084</td>\n",
       "      <td>-0.1958</td>\n",
       "      <td>-0.9806</td>\n",
       "      <td>36.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>15:55:55</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>-0.9243</td>\n",
       "      <td>36.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29242795</th>\n",
       "      <td>29242795</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>10:23:30</td>\n",
       "      <td>-0.0869</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>-0.8478</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29242796</th>\n",
       "      <td>29242796</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>10:23:30</td>\n",
       "      <td>-0.0673</td>\n",
       "      <td>0.3398</td>\n",
       "      <td>-0.8679</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29242797</th>\n",
       "      <td>29242797</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>10:23:30</td>\n",
       "      <td>-0.0595</td>\n",
       "      <td>0.3516</td>\n",
       "      <td>-0.9122</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29242798</th>\n",
       "      <td>29242798</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>10:23:30</td>\n",
       "      <td>-0.0477</td>\n",
       "      <td>0.4415</td>\n",
       "      <td>-0.9525</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29242799</th>\n",
       "      <td>29242799</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>10:23:30</td>\n",
       "      <td>-0.0477</td>\n",
       "      <td>0.5236</td>\n",
       "      <td>-0.9605</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29242800 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0        Date      Time       X       Y       Z     T  \\\n",
       "0                  0  2018-07-27  15:55:55 -0.0359 -0.2505 -1.0169  36.2   \n",
       "1                  1  2018-07-27  15:55:55 -0.1655 -0.1098 -0.8518  36.2   \n",
       "2                  2  2018-07-27  15:55:55 -0.1026 -0.1528 -0.9203  36.2   \n",
       "3                  3  2018-07-27  15:55:55 -0.0084 -0.1958 -0.9806  36.2   \n",
       "4                  4  2018-07-27  15:55:55 -0.0045 -0.2154 -0.9243  36.2   \n",
       "...              ...         ...       ...     ...     ...     ...   ...   \n",
       "29242795    29242795  2018-08-03  10:23:30 -0.0869  0.3985 -0.8478  22.9   \n",
       "29242796    29242796  2018-08-03  10:23:30 -0.0673  0.3398 -0.8679  22.9   \n",
       "29242797    29242797  2018-08-03  10:23:30 -0.0595  0.3516 -0.9122  22.9   \n",
       "29242798    29242798  2018-08-03  10:23:30 -0.0477  0.4415 -0.9525  22.9   \n",
       "29242799    29242799  2018-08-03  10:23:30 -0.0477  0.5236 -0.9605  22.9   \n",
       "\n",
       "          Sleep  activity_index  \n",
       "0             0        0.157358  \n",
       "1             0        0.157358  \n",
       "2             0        0.157358  \n",
       "3             0        0.157358  \n",
       "4             0        0.157358  \n",
       "...         ...             ...  \n",
       "29242795      0        0.120275  \n",
       "29242796      0        0.120275  \n",
       "29242797      0        0.120275  \n",
       "29242798      0        0.120275  \n",
       "29242799      0        0.120275  \n",
       "\n",
       "[29242800 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final = pd.read_csv('C:/Users/marim/Documents/Faculdade/TCC/patient_act_data_CSV/1379-38_data_final.csv')\n",
    "data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instância de um objeto da classe SleepPy para teste do pacote.\n",
    "\n",
    "Tempo aproximado de execução: 15 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está rodando normalmente: 01/10/24 - 17 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input de dados para leitura de um arquivo csv referente a um unico relogio de uma unica paciente\n",
    "\n",
    "input_file = 'patient_act_data_CSV/1839-39_left wrist_046896_2020-12-18 19-35-29.csv'\n",
    "results_directory = 'C:/Users/marim/Documents/Faculdade/TCC/patient_act_data_CSV'\n",
    "sampling_frequency = 50\n",
    "start_buffer=\"0s\"\n",
    "stop_buffer=\"0s\"\n",
    "start_time=\"\"\n",
    "stop_time=\"\"\n",
    "run_config=0\n",
    "temperature_threshold=25.0\n",
    "minimum_rest_block=30\n",
    "allowed_rest_break=60\n",
    "minimum_rest_threshold=0.0\n",
    "maximum_rest_threshold=1000.0\n",
    "minimum_hours=6\n",
    "clear_intermediate_data=False\n",
    "aws_object=None\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = SleepPy(input_file, \n",
    "                results_directory, \n",
    "                sampling_frequency, \n",
    "                start_buffer, \n",
    "                stop_buffer, \n",
    "                start_time, \n",
    "                stop_time, \n",
    "                run_config, \n",
    "                temperature_threshold, \n",
    "                minimum_rest_block, \n",
    "                allowed_rest_break, \n",
    "                minimum_rest_threshold, \n",
    "                maximum_rest_threshold, \n",
    "                minimum_hours, \n",
    "                clear_intermediate_data, \n",
    "                aws_object, \n",
    "                verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data - Excluir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "    for filename in filenames:\n",
    "\n",
    "        if filename.endswith('.csv') and 'raw_data' in filename:\n",
    "            # Monta o caminho completo do arquivo\n",
    "            csv_file_path = os.path.join(dirpath, filename)\n",
    "            \n",
    "            df_raw = pd.read_csv(csv_file_path, nrows=100)\n",
    "            \n",
    "            print(f\"Arquivo CSV lido: {filename}\")\n",
    "\n",
    "            a = os.path.splitext(os.path.basename(csv_file_path))\n",
    "            patient_id_csv = a[0].split('_')\n",
    "            patient_id_csv[0]\n",
    "\n",
    "        elif filename.endswith('.csv') and 'activity_index' in filename:\n",
    "            # Monta o caminho completo do arquivo\n",
    "            csv_file_path = os.path.join(dirpath, filename)\n",
    "            \n",
    "            df_activity_index = pd.read_csv(csv_file_path, nrows=100)\n",
    "            \n",
    "            print(f\"Arquivo CSV lido: {filename}\")\n",
    "        \n",
    "        elif filename.endswith('.csv') and 'wear_detection' in filename:\n",
    "            # Monta o caminho completo do arquivo\n",
    "            csv_file_path = os.path.join(dirpath, filename)\n",
    "            \n",
    "            df_wear_detection = pd.read_csv(csv_file_path, nrows=100)\n",
    "            \n",
    "            print(f\"Arquivo CSV lido: {filename}\")\n",
    "        \n",
    "        elif filename.endswith('.csv') and 'ck_predictions' in filename:\n",
    "            # Monta o caminho completo do arquivo\n",
    "            csv_file_path = os.path.join(dirpath, filename)\n",
    "            \n",
    "            df_ck_predictions = pd.read_csv(csv_file_path, nrows=100)\n",
    "            \n",
    "            print(f\"Arquivo CSV lido: {filename}\")\n",
    "    \n",
    "\n",
    "df_raw_f, df_activity_index_f, df_wear_detection_f, df_ck_predictions_f = mgd.df_format(df_raw, df_activity_index, df_wear_detection, df_ck_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ck_predictions = df_ck_predictions.rename(columns={'Time': 'Datetime'})\n",
    "df_ck_predictions[\"Datetime\"] = pd.to_datetime(df_ck_predictions[\"Datetime\"])\n",
    "\n",
    "# Extraindo a data e formatando o horário até os segundos\n",
    "df_ck_predictions['Date'] = df_ck_predictions[\"Datetime\"].dt.date\n",
    "df_ck_predictions['Time'] = df_ck_predictions[\"Datetime\"].dt.strftime('%H:%M:%S')\n",
    "\n",
    "#df_activity_index = df_activity_index.drop(columns=[\"Time\"])\n",
    "df_ck_predictions = df_ck_predictions[['Date', 'Time', 'sleep_predictions']]\n",
    "\n",
    "df_ck_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity_index = df_activity_index.rename(columns={'Time': 'Datetime'})\n",
    "df_activity_index[\"Datetime\"] = pd.to_datetime(df_activity_index[\"Datetime\"])\n",
    "\n",
    "# Extraindo a data e formatando o horário até os segundos\n",
    "df_activity_index['Date'] = df_activity_index[\"Datetime\"].dt.date\n",
    "df_activity_index['Time'] = df_activity_index[\"Datetime\"].dt.strftime('%H:%M:%S')\n",
    "\n",
    "#df_activity_index = df_activity_index.drop(columns=[\"Time\"])\n",
    "df_activity_index = df_activity_index[['Date', 'Time', 'activity_index']]\n",
    "\n",
    "df_activity_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wear_detection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_sum_cut = sleep_sum[['ID', 'calendar_date', 'sleeponset_ts', 'wakeup_ts']]\n",
    "sleep_sum_cut.calendar_date = pd.to_datetime(sleep_sum_cut.calendar_date)\n",
    "sleep_sum_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"Unnamed: 0\"] = pd.to_datetime(df_raw[\"Unnamed: 0\"])\n",
    "\n",
    "# Extraindo a data e formatando o horário até os segundos\n",
    "df_raw['Date'] = df_raw[\"Unnamed: 0\"].dt.date\n",
    "df_raw['Time'] = df_raw[\"Unnamed: 0\"].dt.strftime('%H:%M:%S')\n",
    "\n",
    "df_raw = df_raw.drop(columns=[\"Unnamed: 0\"])\n",
    "df_raw = df_raw[['Date', 'Time', 'X', 'Y', 'Z', 'T', 'Sleep']]\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_sleep_sum = sleep_sum_cut[sleep_sum_cut['ID'] == patient_id_csv[0]] # deve procurar por todas as pacientes\n",
    "patient_sleep_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando sobre cada linha da tabela result\n",
    "for _, row in patient_sleep_sum.iterrows():\n",
    "    calendar_date = row['calendar_date']\n",
    "    sleeponset_ts = row['sleeponset_ts']\n",
    "    wakeup_ts = row['wakeup_ts']\n",
    "    \n",
    "    # Filtrando a segunda tabela pela data correspondente\n",
    "    mask = df_raw['Date'] == calendar_date\n",
    "    date_filtered_df = df_raw[mask]\n",
    "\n",
    "    # Encontrar o indice \n",
    "    sleeponset_idx = date_filtered_df[date_filtered_df['Time'] == sleeponset_ts].index\n",
    "    wakeup_idx = date_filtered_df[date_filtered_df['Time'] == wakeup_ts].index\n",
    "\n",
    "    if not sleeponset_idx.empty and not wakeup_idx.empty:\n",
    "        df_raw.loc[sleeponset_idx[0]:wakeup_idx[0], 'Sleep'] = 1\n",
    "\n",
    "# Exibindo o resultado final\n",
    "print(df_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_raw, df_activity_index, how='left', on=['Date', 'Time'])\n",
    "\n",
    "# Propagar os valores de activity_index para frente (até o próximo minuto)\n",
    "merged_df['activity_index'] = merged_df['activity_index'].fillna(method='ffill')\n",
    "\n",
    "merged_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
